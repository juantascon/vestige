==========
2009/06/12
==========


Starting with the simplest example from Osgart toolkit
(osgartsimple.cpp), I made some performance tests to measure the
behavior in different kinds of situations, in particular on
limit conditions like half views (splitting) of the markers
and high inclinations of the camera with respect to the markers
positions.

The results were better than expected in the case of high
inclinations, as the markers can be recognized even on 180 degrees
scenarios. However, in the half views tests, the markers
could not be recognized. This will probably be a problem and
in the future some solutions should be proposed.


==========
2009/06/19
==========


The simple example provided with the Osgart toolkit was
improved. I added another marker to the setting in order to
evaluate the behavior in this case.  The toolkit was able to
differentiate between the two markers even when they were in the
same video frame

I investigated how to execute a process on every iteration of the
marker recognition procedure. I found out that the
implementation was actually based on a callback for debugging
purposes (MarkerDebugCallback). It was tricky to find how the
Osgart callbacks system works due to the lack of documentation
about the development version of the toolkit.

Using this discovered callback mechanism I created a simple demo
that determines the distance between two markers.


==========
2009/06/26
==========


This week, I tried to fix the problem of the coordinates changes
from the camera point of view; when the camera is moving the view-port
is changing and the coordinates of the located markers are changing
too.

The markers are recognized with a coordinate system relative to the
camera position and this represents an obstacle when we try to get
information like the exact distance or the level of alignment between
two markers.

This problem turn out to be more complicated that I expected so after
reading some examples on-line (1*) and running some concept tests I
concluded that the best solution --at least for now-- would be to
ignore this phenomenon because we can use ranges when we compare the
coordinates among the markers, this is possible due to the low
probability that the camera reaches a point of high inclination
relatively to the markers position, so the coordinates change
enough to be out of range.

Another more simplistic, but at the same time more radical
solution, would be to use a static camera: this would cut the
coordinates problem at the root and, considering the inconvenience
of the range system, this seems a good trade-off.

*** References:

1. Interaction Tutorial 4: Multiple Marker Proximity, http://www.osgart.org/node/23


==========
2009/07/10
==========


I finished the first demo. The program handles internally two
kinds of marker blocks and lists. A list is a special block in
charge of a group of blocks and a block is a normal
marker that represents a data item, which could be variable names
or constant data. The biggest challenge this week was to debug the
C++ code because it has been a long time since I have not
worked on a considerable amount of C++ code and I had to review
some common issues.

I decided to use git (1*) as my revision[-]control system, [starting
at] the end of this version.

*** References:

1. Git website, http://git-scm.com/


==========
2009/09/04
==========


This week, my work was devoted to establish the state of the art
in gesture recognition software, in order to improve the Human
Computer Interface of our system. The first approach will be to
use OpenCV functions capabilities. OpenCV (Open Source Computer
Vision) is a library of programming functions mainly aimed at real
time computer vision. (1*)

After gathering and analyzing information on different approaches such
as

* O.G.R.E. -- Open Gestures Recognition Engine (2*)
* Computer Vision Based Human-Computer Interaction (3*)
* IGesture (4*)
* Gesture Recognition for Human Machine Interfaces (5*)
* The HandVu vision-based hand gesture interface (6*)

I decided to try HandVu mainly because (1) it is the the only
library in C++ whose source code is open source (GPL2) and
IGesture is also free (but only available on Java platforms);
(2) it is compatible with ARToolkit, one of the libraries used for
the marker recognition process.

<Begin personal note> It is disappointing to realise that in
Computer Science, many investigators don't publish the
resulting application or library source code, impeding further
investigations by other researchers.<End personal note>

The available HandVu source code was reluctant to compile on the
Linux operating system, in particular due to some changes in the
libraries requiring GCC version 4. These fixes are kept in a
series of patches that I will try to submit to the developers.

Next week I will try to execute the available examples included in
the HandVu package and also I will study the library API and some
of its internal features.

*** References:

1. http://opencv.willowgarage.com/wiki/FullOpenCVWiki#Welcome.2BAC8-Introduction.WhatisOpenCV.3F
2. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.59.4787
3. http://www.nada.kth.se/cvap/gvmdi/index.html
4. http://www.igesture.org
5. http://paloma.isr.uc.pt/gesture-reco/index.html
6. http://ilab.cs.ucsb.edu/projects/mathias/handvu_ilab.html


==========
2009/09/11
==========

I started my week by contacting HandVu developers in order to send them the generated
patches; after a couple of email exchanges they notify me that the library is not
currently under develop, which means that probably there will be no stable release
(the current release is a beta version).

These days Ive tried to run the provided examples with no good results, I couldn't
see any of the demos in action.

Due to all these inconveniences I will focus next week in propose a new HCI method
that allow the system to differentiate among the different states (snapshots) of
the captured environment, as mentioned before this is a very important step in the
communication between the user and the system because the user have to inform the
system that he has made changes in the environment and that he wants to commit
these changes.


==========
2009/09/18
==========

My progress this week has not been very notorious, the main conclusion is that
we are considering not to use hand recognition in the stage recognition process,
we will evaluate if it's possible to use the visibility property of one of the
markers, the user/developer will cover a specially located marker to inform
the system that the current state represents the next stage of his abstract
representation of the problem. 

To get that, we'll run some tests to gather information related to the
continuity of the markers recognition flow, we will add basically a debugging
system that prints frame by frame the current recognized environment, after that
we will run similar tests with videos that represents a possible user
interaction, this is, a user hiding the special marker and the current
recognized state.

We contacted one of the VIP Laboratory's students and her feedback was that
their projects are only related with face recognition techniques; I tried,
however, to contact professor Eun Yi Kim (the professor in charge of VIP
Laboratory) about another possible projects in his Laboratory that could be
useful in our research but I haven't had any answer yet, it seems like there
isn't enough developed material that we can reuse.

The initial idea is still opened, if we receive a positive answer from professor
Eun Yi Kim we will reconsider to use hand recognition in the state recognition
procedure.


==========
2009/09/25
==========

After a couple days of work I finally could execute the HandVu library demos,
the changes that I made -a part of the previous patches to compile on gcc4.4
systems- were stop using the ARToolkit capture method and instead, I started
using OpenCV capture capabilities.

I had some initial problems problems with OpenCV as well because the
cvCaptureFromFile method was returning null value, after a couple of hours
searching I discovered that there was a new beta release for Linux systems with
some new features, among this features was the addition of GStreamer Framework
as capturing interface.

According with their website (*1) GStreamer is a library for constructing graphs
of media-handling components, this means that applications can take advantage of
advances in codec and filter technology transparently, GStreamer is a very
stable library included in many Linux distributions.

This new capturing interface solved all the HandVu demos execution problems and
also added the capabilities to all the video input formats and devices supported
by GStreamer.


*** References:

1. GStreamer website: http://gstreamer.freedesktop.org/


==========
= Week 40
==========

It was very frustrating to finally compile and execute the demos provided by the
HandVu library but at the same time not to be able to make it run in the
expected way by keeping the minimum requirements such as good hand recognition
results and a decent execution speed having a modest overhead in the system's
general load.

The tracker was unable to identify the shape of my hand, we tried executing it
in different backgrounds and environments without getting better results, after
a couple of runnings the binaries refused to execute because of problems in the
OpenCV Library (we are using a beta version of this library) procedures.

I experimented very low frame rates during the tracking process and after a
quick search in the HandVu website (1*) I discovered, that the Linux version
doesn't have good tracking speed with high frame rates, this was planned as one
of the "coming soon" features for future releases but this features never became
real.   

Our conclusion for this week is to continue with the secondary plan that was to
use the visibility property of a special marker, when the marker gets covered
bye the user/developer the system will understand this action as a change of
the current state, taking a snapshot of the current representation of the
problem.

As an extra point, the ability to show any string on top of the markers has been
added to the project's TO-DO list. 


*** References:

1. HandVu website: http://www.movesinstitute.org/~kolsch/HandVu/HandVu.html


==========
= Week 42
==========

My work this week is very notorious, mainly because the output is visible
progress in the development of the software (new features).

I made a redesign in the code for better performance, in the old days the
position recognition process was done by a single callback in charge of analyze
which markers were align with each other, this was changed to new callbacks
in the scope of each list and each block (a block and a list are specializations
of markers) with this the process will only have to iterate through the markers
whose position have changed.

I finally build a good debug interface, we can now print on screen in real-time
the current state of the recognized system all the lists and their aligned
blocks.

And the most visual feature of all, finally, the ability to show a marker as a
variable, this is, on top of each marker we show a string with the form
"key = value", key representing the name of the variable and value as its name
indicates the value of the variable.

Additionally to this I contacted designer JanEun Cho in order to coordinate with
her our designs needs, I found the plug-in to make 3d studio max (her design
tool) to export in osg format (the format accepted by osgart Toolkit) as
previously mentioned this was a hitch for us to use her designs.

The next week I want to continue with the improvement of some of this new
features, for instance right now when we show the variable string on the marker
we are unable to show the 3d model that represents the type of marker, the
debugging system doesn't print properly blocks that aren't aligned with any list
marker, and, if I have time, I will start with the development of the special
marker that will indicates states changes.

As a personal comment I would like to say that I feel an important improvement
on my English writing skills, this little reports use to take me almost a full
afternoon but now I finish them in about 60-90 minutes and I can successfully
put on the report all of my ideas without the previous feeling that I'm leaving
something out.


==========
= Week 43
==========

There is not much to say about what I did this week except that I Improved the
features built last week, the debug interface had been fixed so that now it can
print correctly blocks and lists, the floating labels (the ones that indicates
the variable name and value) are now only in blocks scope, they can also be
shown altogether with the 3d model.

I also checked some example codes of OpenSceneGraph because I'm thinking to
change the way that lists are displayed using some variable indicator that will
be put on top of the child blocks showing only the very last item of the list.

My plan for the next week is to implement the marker that will work as state
change notifier.


==========
= Week 45
==========

- SICK/FEVER -


==========
= Week 45
==========

My work this week was focused on programming, I had redesigned the application
making a few changes in the internal structure of the code, there was an overall
redesign of the way the information is stored.

The first change is that there is a new singleton class called EContainer in
charge of register all the lists and blocks in the system, this way a block can
relocate its logical position -when its physical position change- by iterating
through all the lists in order to check which one is aligned with it and
dynamically adding itself to the logic internal structure of the list, this
fixed a little bug opened when I changed the callbacks from a default one to a
specialized one on each list and block scopes.

Another new singleton class is the EFactory class, in charge of create all the
OSGart related object such as Video, Tracker, Calibration, Camera, Viewer, etc.
It also stores these objects in a global environment so that other classes can
have instant access to this information, for now the only data used is the
Tracker object used in the creation of markers.

The lists and blocks are now more specialized objects, only blocks have an
internal label object, blocks now know how to print themselves so the lists
fallback on this ability.

One last minor change was the renaming of the header files by changing the
extension from .h to the more modern and highly recommended .hpp.

I'm planning to do an even more important redesign by adding the concept of
state, so part of my work next week might be on the design and not so much on
the programming.
